Bảng so sánh sau đây làm nổi bật sự khác biệt và chức năng của từng dịch vụ AWS AI:
Dịch vụ
Mục đích chính
Đối tượng người dùng
Mức độ kỹ thuật
Lợi ích cốt lõi
Amazon SageMaker JumpStart
Tăng tốc hành trình ML, triển khai mô hình có sẵn
Nhà khoa học dữ liệu, kỹ sư ML
Chuyên nghiệp (Pro-code)
Tùy chỉnh mô hình, an toàn dữ liệu trong VPC
Amazon Bedrock
Xây dựng và mở rộng ứng dụng AI tạo sinh
Nhà phát triển, doanh nghiệp
Thấp đến cao (Low-code/Pro-code)
Lựa chọn mô hình đa dạng, bảo vệ tích hợp
PartyRock
Môi trường học tập và thử nghiệm AI tạo sinh
Mọi người, người mới bắt đầu
Không cần mã (No-code)
Rào cản gia nhập thấp, học prompt engineering
Amazon Q
Trợ lý AI cho doanh nghiệp
Nhân viên doanh nghiệp, nhà phát triển
Không cần mã đến tùy chỉnh (No-code/Custom)
Tăng năng suất, thông tin thống nhất, an toàn


3.2. Lợi Ích của Việc Sử Dụng Các Dịch Vụ AI Tạo Sinh của AWS

Việc xây dựng các ứng dụng AI tạo sinh trên nền tảng AWS mang lại nhiều lợi thế cạnh tranh [1, 4]:
Khả năng tiếp cận và Rào cản gia nhập thấp: Các dịch vụ được quản lý toàn phần như Bedrock và PartyRock giúp người dùng và doanh nghiệp dễ dàng bắt đầu với AI tạo sinh mà không cần đầu tư lớn vào cơ sở hạ tầng hoặc đội ngũ chuyên gia [1, 12].
Hiệu quả và Tốc độ ra thị trường: AWS giúp doanh nghiệp tập trung vào việc phát triển ứng dụng thay vì quản lý cơ sở hạ tầng phức tạp, từ đó tăng tốc độ đưa sản phẩm ra thị trường [34].
Khả năng đáp ứng mục tiêu kinh doanh: Bằng cách cung cấp một loạt các mô hình và công cụ, AWS cho phép doanh nghiệp tùy chỉnh giải pháp để giải quyết các vấn đề kinh doanh cụ thể, từ đó tạo ra giá trị kinh doanh thực sự [36].

3.3. Lợi Ích của Cơ Sở Hạ Tầng AWS cho Ứng Dụng AI Tạo Sinh

Các dịch vụ AI tạo sinh của AWS chỉ có thể hoạt động hiệu quả nhờ vào sức mạnh của cơ sở hạ tầng toàn cầu AWS [37]. Nền tảng hạ tầng này cung cấp:
Bảo mật, Tuân thủ và An toàn: AWS tích hợp bảo mật từ lớp hạ tầng cốt lõi [37]. Mọi lưu lượng dữ liệu trên mạng toàn cầu của AWS đều được mã hóa tự động [37]. AWS cũng cung cấp các công cụ như Bedrock Guardrails [15] và tuân thủ các tiêu chuẩn quốc tế như ISO, SOC, GDPR và HIPAA [15], trực tiếp giải quyết các lo ngại về bảo mật và trách nhiệm đã được đề cập [1, 38].
Hiệu năng và Khả năng mở rộng quy mô (Performance & Scalability): Hạ tầng của AWS được xây dựng để đạt hiệu năng cao, với độ trễ thấp và tỷ lệ mất gói thấp [37]. Khả năng mở rộng linh hoạt của đám mây cho phép doanh nghiệp điều chỉnh tài nguyên theo nhu cầu, tránh lãng phí [37].

3.4. Cân Bằng Chi Phí (Cost Tradeoffs) của Dịch Vụ AWS

Việc lựa chọn mô hình định giá phù hợp là một quyết định chiến lược, đòi hỏi sự cân bằng giữa chi phí, hiệu suất và tính linh hoạt [39]. AWS cung cấp hai mô hình định giá chính cho AI tạo sinh:
Mô hình định giá dựa trên Tokens (On-Demand Pricing):
Khái niệm: Thanh toán theo mức sử dụng thực tế, tính phí trên mỗi token đầu vào và đầu ra [8, 39]. Một token là đơn vị văn bản cơ bản mà mô hình xử lý [39].
Ưu điểm: Linh hoạt, không cần cam kết trả trước [8]. Lý tưởng cho các khối lượng công việc thử nghiệm hoặc không thể đoán trước.
Thực tế: Có một mối liên kết trực tiếp giữa kỹ năng prompt engineering và chi phí. Bằng cách tối ưu hóa câu lệnh để giảm số lượng token, người dùng có thể tiết kiệm đáng kể chi phí [8]. Điều này làm cho prompt engineering không chỉ là một kỹ năng kỹ thuật mà còn là một kỹ năng quản lý chi phí [8].
Mô hình Thông lượng được cung cấp (Provisioned Throughput):
Khái niệm: Mua trước một dung lượng nhất định dưới dạng các đơn vị mô hình [39, 40]. Tính phí theo giờ, với các tùy chọn cam kết 1 hoặc 6 tháng [39, 40].
Ưu điểm: Đảm bảo hiệu suất và độ sẵn sàng cho các khối lượng công việc lớn, ổn định [8].
Nhược điểm: Yêu cầu cam kết, kém linh hoạt hơn so với on-demand [8].
Tiêu chí
Định giá theo Tokens (On-Demand)
Thông lượng được cung cấp (Provisioned Throughput)
Cách tính phí
Trên mỗi token đầu vào và đầu ra
Theo giờ cho mỗi đơn vị mô hình
Tính linh hoạt
Rất cao, không có cam kết
Thấp, yêu cầu cam kết 1 hoặc 6 tháng
Trường hợp sử dụng
Thử nghiệm, khối lượng công việc không ổn định
Khối lượng công việc lớn, ổn định, cần thông lượng đảm bảo


Kết Luận

Qua việc phân tích chuyên sâu các nguyên lý, khả năng, và công nghệ liên quan đến AI tạo sinh, tài liệu này đã cung cấp một lộ trình học tập toàn diện cho Domain 2 của kỳ thi chứng chỉ AWS Certified AI Practitioner. Các khái niệm cốt lõi như tokens, embeddings, và kiến trúc Transformer tạo thành nền tảng kỹ thuật cho toàn bộ lĩnh vực này, trong khi prompt engineering nổi lên như một kỹ năng then chốt để điều khiển các mô hình.
Báo cáo này cũng nhấn mạnh rằng việc triển khai AI tạo sinh không chỉ là một vấn đề kỹ thuật mà còn là một quyết định kinh doanh chiến lược. Các doanh nghiệp cần hiểu rõ cả ưu điểm (tính thích ứng, năng suất) và nhược điểm (ảo giác, thiếu diễn giải) để đưa ra các quyết định sáng suốt. Giá trị của AI tạo sinh cần được đo lường bằng các chỉ số kinh doanh cụ thể, không chỉ là hiệu suất mô hình.
Cuối cùng, AWS đã định vị mình là một đối tác đáng tin cậy bằng cách cung cấp một hệ sinh thái dịch vụ hoàn chỉnh (Bedrock, SageMaker JumpStart, PartyRock, Amazon Q) được xây dựng trên một cơ sở hạ tầng toàn cầu mạnh mẽ, đảm bảo bảo mật, tuân thủ và khả năng mở rộng quy mô. Sự hiểu biết về các mô hình định giá khác nhau của AWS (dựa trên token và thông lượng được cung cấp) là yếu tố then chốt để quản lý chi phí hiệu quả và đưa các ứng dụng AI tạo sinh vào sản xuất một cách bền vững.

Tài liệu học tập chuyên sâu cho AWS Certified AI Practitioner: Domain 3 - Ứng dụng các Mô hình Nền tảng

AWS Certified AI Practitioner là một chứng chỉ được thiết kế để xác nhận kiến thức của các cá nhân về AI, Machine Learning (ML), và AI tạo sinh, cùng với các dịch vụ AWS liên quan.1 Bài thi chứng chỉ này không yêu cầu một vai trò công việc cụ thể, nhưng nhắm đến những người có kinh nghiệm khoảng 6 tháng làm quen với các công nghệ AI/ML trên AWS.1 Các ứng viên thường là chuyên gia phân tích kinh doanh, quản lý dự án, hoặc các chuyên gia marketing, những người sử dụng các giải pháp này mà không nhất thiết phải tự xây dựng chúng.2
Domain 3, “Applications of Foundation Models,” chiếm trọng số 28% tổng số điểm của bài thi, là phần có tỷ lệ cao nhất.1 Điều này nhấn mạnh tầm quan trọng của việc hiểu cách thiết kế, xây dựng, và đánh giá các ứng dụng AI tạo sinh dựa trên mô hình nền tảng (Foundation Models - FMs) trong môi trường thực tế. Tài liệu này cung cấp một cái nhìn toàn diện và chuyên sâu về các chủ đề của Domain 3, tập trung vào Amazon Bedrock, một dịch vụ trung tâm giúp đơn giản hóa việc phát triển các ứng dụng AI tạo sinh.3 Amazon Bedrock cung cấp quyền truy cập vào một loạt các mô hình hàng đầu từ các nhà cung cấp như AI21 Labs, Anthropic, Cohere, Meta, và Stability AI, cũng như các mô hình Amazon Titan do AWS phát triển.4 Việc sử dụng một API duy nhất của Bedrock giúp giảm bớt đáng kể gánh nặng quản lý cơ sở hạ tầng, cho phép các nhà phát triển tập trung vào việc xây dựng các ứng dụng có giá trị kinh doanh cao.3

Phần 1: Các Cân Nhắc Thiết Kế Ứng Dụng Mô Hình Nền tảng (Domain 3.1)


Tiêu chí lựa chọn Mô hình Nền tảng

Việc lựa chọn một mô hình nền tảng phù hợp là một quyết định chiến lược, không chỉ đơn thuần là kỹ thuật. Mặc dù các tổ chức thường bắt đầu với ba tiêu chí chính là độ chính xác (accuracy), độ trễ (latency), và chi phí (cost), một khung đánh giá toàn diện hơn là cần thiết để xác định hiệu suất thực tế của mô hình trong thế giới thực.6 Quá trình này cần xem xét bốn khía cạnh cốt lõi.
Hiệu suất Tác vụ (Task Performance): Đánh giá khả năng của mô hình trong việc thực hiện các nhiệm vụ cụ thể. Điều này bao gồm khả năng tuân thủ hướng dẫn một cách chính xác (instruction following fidelity), tính nhất quán của đầu ra qua nhiều lần chạy với cùng một prompt (output consistency), và sự hiểu biết chuyên sâu về một miền cụ thể (domain-specific knowledge) dựa trên dữ liệu đào tạo.6 Khả năng suy luận (reasoning capabilities) cũng rất quan trọng, đặc biệt là khả năng thực hiện suy luận logic, suy luận nhân quả, và giải quyết vấn đề nhiều bước.6
Đặc điểm Kiến trúc (Architectural Characteristics): Các yếu tố kỹ thuật như số lượng tham số (parameter count hoặc model size) ảnh hưởng trực tiếp đến hiệu suất và hiệu quả.6 Các mô hình lớn hơn thường có nhiều khả năng hơn nhưng cũng đòi hỏi tài nguyên tính toán lớn hơn, dẫn đến chi phí và độ trễ suy luận cao hơn.6 Cần xem xét thành phần dữ liệu đào tạo (training data composition), loại hình (modality) mà mô hình có thể xử lý (ví dụ: văn bản, hình ảnh, âm thanh), và kích thước cửa sổ ngữ cảnh (context window size).6 Các kiến trúc khác nhau như
decoder-only (tối ưu cho tạo văn bản) và encoder-decoder (phù hợp cho dịch thuật và tóm tắt) cũng cần được cân nhắc.6
Cân nhắc Vận hành (Operational Considerations): Đây là các yếu tố quyết định tính khả thi và hiệu quả chi phí của một triển khai AI trong thực tế. Chúng bao gồm thông lượng (throughput) và độ trễ (latency), cấu trúc chi phí (cost structure) theo token hoặc thông lượng dự phòng (provisioned throughput), các tùy chọn tùy chỉnh (customization options), và khả năng tích hợp dễ dàng vào hệ thống hiện có (ease of integration).6 Vấn đề bảo mật, bao gồm mã hóa dữ liệu, kiểm soát truy cập và quản lý lỗ hổng, là một cân nhắc quan trọng khi xử lý dữ liệu nhạy cảm.6
Thuộc tính AI có Trách nhiệm (Responsible AI Attributes): Một yếu tố không thể thiếu trong kinh doanh hiện đại là trách nhiệm. Điều này đòi hỏi phải đánh giá khuynh hướng tạo ra thông tin sai lệch (hallucination propensity), đo lường độ thiên lệch (bias measurements), hiệu quả của các rào chắn an toàn (safety guardrails) trong việc ngăn chặn nội dung độc hại, và các khía cạnh về quyền riêng tư và pháp lý.6
Lựa chọn mô hình không chỉ là một bài toán kỹ thuật mà còn là một quyết định chiến lược. Một ứng dụng yêu cầu độ chính xác tuyệt đối trong lĩnh vực pháp lý sẽ ưu tiên các mô hình có kiến thức chuyên ngành sâu, trong khi một chatbot hỗ trợ khách hàng sẽ cần các mô hình có khả năng tuân thủ hướng dẫn và độ trễ thấp.6 Quyết định này sẽ định hình toàn bộ kiến trúc và chi phí của giải pháp. Do đó, việc xác định chính xác các yêu cầu chức năng và phi chức năng của ứng dụng ở giai đoạn đầu là rất quan trọng để đưa ra quyết định đúng đắn.6

Tác động của các tham số suy luận (Inference Parameters)

Các tham số suy luận cho phép điều chỉnh hành vi của mô hình mà không cần phải đào tạo lại. Hai trong số các tham số quan trọng nhất là temperature và input/output length.
Temperature: Tham số này kiểm soát tính ngẫu nhiên và sáng tạo của đầu ra bằng cách điều chỉnh phân phối xác suất của các token.7 Một giá trị
temperature thấp (ví dụ: nhỏ hơn 1) làm cho mô hình trở nên "bảo thủ" hơn, ưu tiên các token có xác suất cao nhất. Điều này tạo ra các câu trả lời mang tính dự đoán, chính xác và nhất quán, lý tưởng cho các tác vụ yêu cầu tính xác thực cao như tóm tắt văn bản hoặc trả lời câu hỏi dựa trên tài liệu.7 Ngược lại, một giá trị
temperature cao (ví dụ: lớn hơn 1) làm phẳng phân phối xác suất, cho phép mô hình có nhiều cơ hội chọn các token ít phổ biến hơn. Điều này khuyến khích sự sáng tạo và đa dạng, phù hợp cho các tác vụ như viết kịch bản, làm thơ, hoặc tạo nội dung quảng cáo.7
Chiều dài Input/Output: Chiều dài đầu vào (prompt length), được đo bằng token, là một yếu tố ảnh hưởng trực tiếp đến đầu ra của mô hình.9 Các prompt ngắn thường tạo ra phản hồi súc tích và tập trung, trong khi các prompt dài hơn có thể tạo ra các phản hồi toàn diện và chi tiết hơn.9 Tuy nhiên, một prompt quá dài có thể làm mô hình trở nên quá tải hoặc tạo ra thông tin không liên quan.9 Chiều dài đầu ra cũng là một tham số quan trọng, giới hạn số lượng token mà mô hình có thể tạo ra. Một quan sát đáng chú ý là khi độ dài đầu vào tăng lên, khả năng suy luận của nhiều mô hình có thể bị suy giảm.10 Thay vì suy luận từng bước một cách logic, mô hình có thể có xu hướng đưa ra câu trả lời trước rồi mới suy luận sau. Điều này làm giảm khả năng trích xuất các sự kiện chính và xây dựng một chuỗi suy luận chặt chẽ của mô hình.10 Điều này cho thấy việc cung cấp quá nhiều thông tin không phải lúc nào cũng tốt. Do đó, các kỹ thuật như RAG và Chain-of-Thought (CoT) trở nên cần thiết, vì chúng giúp mô hình tập trung vào thông tin quan trọng nhất, thay vì cố gắng xử lý toàn bộ tài liệu cùng một lúc.11

Retrieval Augmented Generation (RAG)

Retrieval Augmented Generation (RAG) là một kỹ thuật mạnh mẽ giải quyết một trong những hạn chế lớn nhất của các mô hình nền tảng: kiến thức tĩnh.13 RAG hoạt động bằng cách truy xuất dữ liệu từ các nguồn bên ngoài, chẳng hạn như cơ sở dữ liệu nội bộ của công ty, và sử dụng dữ liệu đó để bổ sung (augment) cho prompt ban đầu.13 Quá trình này giúp mô hình truy cập được thông tin cập nhật và độc quyền, từ đó tạo ra các phản hồi chính xác và phù hợp hơn, đồng thời giảm đáng kể hiện tượng "hallucinations" (mô hình tạo ra thông tin sai lệch hoặc không có thật).5
Amazon Bedrock Knowledge Bases là một dịch vụ quản lý hoàn toàn (fully managed) giúp triển khai quy trình RAG một cách liền mạch.13 Dịch vụ này tự động hóa toàn bộ quy trình từ việc thu nạp dữ liệu từ các nguồn như Amazon S3, Salesforce, và Confluence, đến việc chuyển đổi nội dung thành các khối văn bản (text chunks) và sau đó thành các vector embeddings.13 Các vector embeddings này sau đó được lưu trữ trong một cơ sở dữ liệu vector để tìm kiếm nhanh chóng.13 Amazon Bedrock Knowledge Bases cũng tích hợp sẵn tính năng quản lý ngữ cảnh phiên làm việc và ghi nhận nguồn thông tin, giúp các ứng dụng trở nên đáng tin cậy hơn.13

Lưu trữ Embeddings với Vector Databases trên AWS

Vector databases là các hệ thống lưu trữ được thiết kế chuyên biệt để quản lý và truy xuất các vector embeddings, cho phép thực hiện tìm kiếm ngữ nghĩa (semantic search) và tìm kiếm lân cận (k-nearest neighbor - k-NN) một cách hiệu quả.14 Các dịch vụ AWS đang tích hợp khả năng này vào các sản phẩm cơ sở dữ liệu hiện có để đơn giản hóa kiến trúc ứng dụng.
Amazon OpenSearch Service: Đây là cơ sở dữ liệu vector được AWS khuyến nghị cho Amazon Bedrock.14 Nó cung cấp một giải pháp có khả năng mở rộng cao và hiệu suất mạnh mẽ, hỗ trợ nhiều loại hình tìm kiếm như
hybrid search (kết hợp tìm kiếm từ khóa và vector), multi-modal search, và conversational search.14
Amazon Aurora và Amazon RDS for PostgreSQL: Các dịch vụ cơ sở dữ liệu quan hệ này hỗ trợ vector search thông qua extension mã nguồn mở pgvector.15 Điều này cho phép các nhà phát triển lưu trữ dữ liệu quan hệ truyền thống và vector embeddings trong cùng một cơ sở dữ liệu, tận dụng các công cụ và quy trình làm việc quen thuộc mà không cần quản lý một hệ thống riêng biệt.16
Amazon DocumentDB (với tính tương thích MongoDB): Dịch vụ cơ sở dữ liệu tài liệu này kết hợp sự linh hoạt của dữ liệu JSON với sức mạnh của vector search.18
Amazon Neptune: Là một dịch vụ cơ sở dữ liệu đồ thị, Amazon Neptune hỗ trợ Neptune Analytics để lưu trữ embeddings và thực hiện tìm kiếm hiệu quả trên dữ liệu đồ thị.13
Chiến lược của AWS là tích hợp khả năng vector search vào các dịch vụ cơ sở dữ liệu hiện có.17 Điều này mang lại lợi ích lớn cho nhà phát triển, vì họ có thể sử dụng các công cụ và quy trình quen thuộc để xây dựng các ứng dụng AI tạo sinh, giảm thiểu việc phải học và quản lý các công nghệ mới.17 Cách tiếp cận này cũng cho phép lưu trữ và xử lý dữ liệu vector cùng với dữ liệu truyền thống trong cùng một hệ thống, đơn giản hóa kiến trúc ứng dụng.

Đánh đổi chi phí trong tùy chỉnh mô hình

Việc tùy chỉnh một mô hình nền tảng có thể được thực hiện theo nhiều cách, mỗi cách có một sự đánh đổi khác nhau về chi phí, thời gian và độ phức tạp.
Pre-training (Đắt nhất): Đây là phương pháp tốn kém nhất về chi phí ($$$$), thời gian (months), và tài nguyên tính toán (serious GPU infrastructure). Phương pháp này chỉ dành cho các tổ chức muốn có toàn quyền kiểm soát mô hình và tạo ra các khả năng hoàn toàn mới.19 Chi phí để đào tạo các mô hình tiên tiến có thể lên tới hàng chục, thậm chí hàng trăm triệu đô la.19
Fine-tuning (Vừa phải): Fine-tuning nhanh hơn và rẻ hơn đáng kể so với pre-training ($$ / medium). Phương pháp này phù hợp để tùy chỉnh một mô hình đã có cho một tác vụ hoặc miền cụ thể bằng cách sử dụng một tập dữ liệu nhỏ hơn, đã được gắn nhãn (labeled data).20 Tuy nhiên, fine-tuning vẫn có thể đòi hỏi tài nguyên GPU đáng kể.19
RAG (Rẻ nhất): Retrieval Augmented Generation (RAG) là cách tiếp cận nhanh nhất và hiệu quả chi phí nhất ($ / short) để tùy chỉnh mô hình.19 Nó không thay đổi các trọng số của mô hình mà chỉ cung cấp ngữ cảnh bổ sung từ một nguồn bên ngoài tại thời điểm suy luận.19 Điều này giúp mô hình truy cập thông tin cập nhật mà không cần tốn thời gian và tài nguyên để đào tạo lại.19
In-context Learning: Đây là một phương pháp nhanh chóng và không yêu cầu đào tạo lại mô hình. Nó bao gồm việc cung cấp ví dụ và hướng dẫn trong chính prompt, và có thể được coi là một phần của quy trình RAG.11 Phương pháp này bị giới hạn bởi độ dài context window của mô hình.22
Bảng sau đây tóm tắt sự đánh đổi giữa các phương pháp:
Phương pháp
Mục tiêu chính
Loại dữ liệu
Chi phí/Thời gian
Độ phức tạp
Pre-training
Tạo ra các khả năng mới, kiểm soát hoàn toàn
Lớn, đa dạng, không gắn nhãn
Rất cao/Rất lâu
Rất cao
Fine-tuning
Tối ưu hóa cho tác vụ/miền cụ thể
Nhỏ hơn, có gắn nhãn, chuyên biệt
Vừa phải/Trung bình
Cao
RAG
Tích hợp kiến thức độc quyền/cập nhật
Dữ liệu bên ngoài (không gắn nhãn)
Thấp/Rất nhanh
Trung bình
In-context Learning
Hướng dẫn hành vi trong prompt
Ví dụ trong prompt
Rất thấp/Tức thời
Thấp


Vai trò của Agents trong các tác vụ đa bước

Agents là các ứng dụng AI tạo sinh có khả năng tự động hóa các tác vụ phức tạp, nhiều bước bằng cách sử dụng khả năng suy luận của mô hình nền tảng, API, và các nguồn dữ liệu.23 Thay vì chỉ trả lời một câu hỏi, một agent có thể phân tích một yêu cầu của người dùng, chia nhỏ nó thành một chuỗi các bước logic, gọi các API cần thiết để thực hiện các hành động trong các hệ thống bên ngoài, và thu thập thông tin để hoàn thành nhiệm vụ.23
Amazon Bedrock Agents là một dịch vụ được thiết kế để đơn giản hóa quá trình xây dựng, triển khai và quản lý các tác vụ này.23 Các tính năng chính bao gồm:
Multi-agent Collaboration (Hợp tác đa agent): Cho phép nhiều agent chuyên biệt cùng làm việc trên một tác vụ phức tạp. Một supervisor agent (agent giám sát) sẽ nhận yêu cầu, phân chia công việc cho các subagents (agent con) phù hợp với lĩnh vực chuyên môn của chúng, và sau đó tổng hợp kết quả để đưa ra câu trả lời cuối cùng.24 Điều này giúp đạt được tỷ lệ thành công và độ chính xác cao hơn cho các tác vụ phức tạp, ví dụ như quản lý chiến dịch marketing trên mạng xã hội.24
Memory Retention (Ghi nhớ): Agents có khả năng ghi nhớ các tương tác trong quá khứ, mang lại trải nghiệm người dùng liền mạch và cá nhân hóa hơn cho các tác vụ nhiều bước.23
Code Interpretation (Diễn giải mã): Cho phép agent tự động tạo và thực thi mã trong một môi trường an toàn. Tính năng này giúp giải quyết các yêu cầu phân tích dữ liệu phức tạp, trực quan hóa dữ liệu hoặc giải quyết các bài toán toán học mà trước đây rất khó để thực hiện chỉ bằng khả năng suy luận của mô hình.23

Phần 2: Kỹ thuật Prompt Engineering Hiệu quả (Domain 3.2)


Các Khái niệm và Cấu trúc cơ bản

Prompt engineering là quá trình tạo ra các prompt (đầu vào văn bản) được thiết kế cẩn thận để hướng dẫn các mô hình nền tảng tạo ra đầu ra mong muốn.11 Quá trình này được coi là cả một nghệ thuật và một khoa học, yêu cầu sự hiểu biết sâu sắc về khả năng và hạn chế của mô hình.11 Một prompt hiệu quả thường bao gồm các thành phần chính sau:
Instruction (Hướng dẫn): Đây là các chỉ dẫn rõ ràng và cụ thể về nhiệm vụ cần thực hiện.11 Ví dụ: "Tóm tắt tài liệu sau," hoặc "Viết một email với giọng điệu trang trọng."
Context (Ngữ cảnh): Cung cấp thông tin nền tảng hoặc dữ liệu mà mô hình cần dựa vào để tạo câu trả lời.11 Đây là cốt lõi của RAG và in-context learning, nơi dữ liệu riêng của công ty được đưa vào prompt.13
Negative Prompts (Prompt phủ định): Chỉ định nội dung hoặc phong cách mà mô hình nên tránh.25 Ví dụ: "không bao gồm từ 'bắt buộc'."
Một khái niệm quan trọng khác là model latent space (không gian tiềm ẩn của mô hình), một biểu diễn toán học trừu tượng của tri thức mà mô hình đã học được. Prompt engineering hoạt động bằng cách điều hướng mô hình đến một khu vực cụ thể trong không gian này để tạo ra câu trả lời phù hợp với yêu cầu.

Các Kỹ thuật Prompt Engineering nâng cao

Các kỹ thuật nâng cao giúp vượt qua những hạn chế của các prompt đơn giản và tận dụng tối đa khả năng của mô hình.
Zero-shot Prompting: Đây là kỹ thuật cơ bản nhất, trong đó prompt không bao gồm bất kỳ ví dụ nào về đầu ra mong muốn.12 Kỹ thuật này dựa vào kiến thức có sẵn của mô hình từ giai đoạn đào tạo sơ bộ để tạo ra phản hồi.12
Few-shot Prompting: Kỹ thuật này cung cấp một vài ví dụ về cặp input-output ngay trong prompt.11 Các ví dụ này hoạt động như một "hướng dẫn đào tạo" nhỏ, giúp mô hình hiểu rõ hơn về nhiệm vụ và định dạng đầu ra mong muốn.12 Kỹ thuật này hiệu quả hơn zero-shot cho các tác vụ cụ thể và phức tạp hơn.
Chain-of-Thought (CoT) Prompting: CoT là một kỹ thuật mạnh mẽ cải thiện khả năng suy luận của mô hình bằng cách yêu cầu nó hiển thị các bước giải quyết vấn đề một cách tuần tự trước khi đưa ra câu trả lời cuối cùng.11 Bằng cách này, mô hình sẽ "tư duy từng bước," làm cho quá trình suy luận trở nên minh bạch và có thể diễn giải được hơn.11 Một ví dụ thực tế cho thấy một prompt few-shot CoT có thể hướng dẫn mô hình giải một bài toán toán học phức tạp một cách chính xác hơn, bằng cách yêu cầu nó hiển thị từng phép tính một.11 Kỹ thuật này được chứng minh là vượt trội hơn few-shot thông thường, đặc biệt trong các bài toán đòi hỏi suy luận phức tạp.12

Lợi ích và Best Practices

Prompt engineering mang lại nhiều lợi ích quan trọng cho việc phát triển ứng dụng AI tạo sinh:
Cải thiện chất lượng và độ chính xác: Bằng cách cung cấp hướng dẫn và ngữ cảnh rõ ràng, prompt engineering giúp định hình phản hồi, làm cho đầu ra của mô hình trở nên phù hợp và chính xác hơn với mục tiêu.11
Thử nghiệm và tối ưu hóa: Quá trình tinh chỉnh prompt là một chu kỳ lặp lại, cho phép các nhà phát triển liên tục thử nghiệm và điều chỉnh để tối ưu hóa hiệu suất của ứng dụng.28
Sử dụng Guardrails: Một best practice quan trọng là sử dụng các rào chắn an toàn (guardrails) để áp dụng các chính sách an toàn, lọc nội dung độc hại hoặc nhạy cảm ở cả đầu vào và đầu ra.11 Guardrails for Amazon Bedrock giúp ngăn chặn các prompt không phù hợp trước khi chúng đến được mô hình và lọc các phản hồi không mong muốn trước khi chúng được trả về cho người dùng.3

Rủi ro và Hạn chế Tiềm ẩn

Mặc dù mạnh mẽ, prompt engineering và các ứng dụng LLM cũng tiềm ẩn nhiều rủi ro an ninh nghiêm trọng.
Exposure (Lộ thông tin): Rủi ro này xảy ra khi người dùng vô tình đưa dữ liệu nhạy cảm hoặc độc quyền vào prompt, và thông tin này có thể bị lộ trong các phản hồi sau đó.22
Poisoning (Đầu độc dữ liệu): Đây là một cuộc tấn công vào tính toàn vẹn của mô hình, trong đó kẻ tấn công đưa dữ liệu xấu vào tập dữ liệu đào tạo hoặc fine-tuning để tạo ra các lỗ hổng, cửa hậu (backdoors), hoặc thiên kiến.29 Mục tiêu là làm suy giảm hiệu suất của mô hình, khiến nó tạo ra các đầu ra sai lệch hoặc độc hại.29 Một ví dụ là việc một kẻ tấn công chèn các tài liệu giả mạo vào dữ liệu đào tạo, dẫn đến việc mô hình tạo ra các phản hồi không chính xác.29
Hijacking (Chiếm quyền điều khiển): LLM Hijacking là một cuộc tấn công ở cấp độ cơ sở hạ tầng, nơi kẻ tấn công chiếm đoạt thông tin xác thực đám mây (stolen cloud credentials) để truy cập và sử dụng dịch vụ LLM trái phép.31 Điều này có thể dẫn đến chi phí sử dụng dịch vụ lên tới hàng chục nghìn đô la mỗi ngày và rủi ro đánh cắp dữ liệu nhạy cảm của doanh nghiệp.31 Cuộc tấn công này nhấn mạnh rằng bảo mật cho các ứng dụng LLM không chỉ giới hạn ở cấp độ tương tác người dùng mà còn phải bao gồm bảo mật hạ tầng đám mây.
Jailbreaking (Vượt rào an toàn): Jailbreaking là một hình thức tấn công social engineering (kỹ thuật xã hội) nhằm vượt qua các rào chắn an toàn và đạo đức được tích hợp trong mô hình.33 Các kỹ thuật jailbreaking thường sử dụng các prompt được tạo ra một cách tinh vi để lừa mô hình, ví dụ như thuyết phục mô hình rằng nó đang ở trong một "chế độ bảo trì đặc biệt" nơi các hạn chế an toàn đã bị vô hiệu hóa.33
Sự kết nối giữa các loại rủi ro này cho thấy một chiến lược an toàn toàn diện là cần thiết. Mặc dù các công cụ bảo vệ model như Amazon Bedrock Guardrails có thể ngăn chặn các cuộc tấn công jailbreaking 11, chúng không thể bảo vệ khỏi một cuộc tấn công hijacking ở cấp độ hạ tầng.31 Một tổ chức có thể có các guardrails mạnh mẽ, nhưng nếu thông tin đăng nhập AWS của họ bị đánh cắp, kẻ tấn công vẫn có thể chiếm quyền điều khiển LLM và gây ra thiệt hại đáng kể.31 Do đó, việc áp dụng các biện pháp bảo mật đám mây truyền thống như quản lý danh tính và quyền truy cập (IAM) chặt chẽ, quản lý bí mật (secrets management), và nguyên tắc đặc quyền tối thiểu (least privilege) là vô cùng quan trọng để bảo vệ tài khoản và dịch vụ của mình.32

Phần 3: Quy trình Training và Fine-tuning (Domain 3.3)


Các Yếu tố Chính của Quy trình Đào tạo

Quy trình đào tạo một mô hình nền tảng bao gồm một số giai đoạn chính, mỗi giai đoạn có một mục tiêu riêng.
Pre-training (Đào tạo sơ bộ): Đây là giai đoạn đầu tiên, trong đó một mô hình được đào tạo trên một lượng dữ liệu tổng quát, không gắn nhãn (unlabeled data) khổng lồ.35 Mục tiêu là giúp mô hình học các quy tắc cơ bản về ngữ pháp, cấu trúc ngôn ngữ và kiến thức phổ quát, tạo ra một nền tảng vững chắc cho các tác vụ sau này.35
Fine-tuning (Tinh chỉnh): Sau khi đã được pre-train, mô hình được tinh chỉnh trên một tập dữ liệu nhỏ hơn, có gắn nhãn (labeled data) để thích nghi với một tác vụ hoặc một miền kiến thức chuyên biệt.4 Ví dụ, một mô hình chung có thể được fine-tune trên dữ liệu khiếu nại của khách hàng để trở thành một hệ thống phân loại chuyên biệt.36
Continuous Pre-training (Đào tạo sơ bộ liên tục): Phương pháp này nằm ở giữa pre-training và fine-tuning.37 Nó là quá trình tiếp tục đào tạo một mô hình đã có trên một tập dữ liệu lớn nhưng chưa gắn nhãn, chuyên biệt cho một miền cụ thể (ví dụ: tài liệu y khoa, văn bản pháp lý).20 Mục đích của phương pháp này là để mô hình làm quen với từ vựng, ngữ pháp và cấu trúc đặc trưng của miền đó, giúp mô hình "nói cùng một ngôn ngữ" với domain trước khi thực hiện fine-tuning.20
Một điểm khác biệt quan trọng cần được làm rõ là sự khác nhau giữa fine-tuning và continuous pre-training. Fine-tuning chủ yếu nhằm cải thiện hiệu suất kỹ thuật cho một TÁC VỤ cụ thể (ví dụ: phân loại văn bản, tóm tắt) bằng cách sử dụng dữ liệu có nhãn.20 Ngược lại, continuous pre-training nhằm mục đích
đưa kiến thức MIỀN CHUYÊN BIỆT vào mô hình (ví dụ: các thuật ngữ tài chính, ngôn ngữ pháp lý) bằng cách sử dụng dữ liệu không nhãn.20 Do đó, lựa chọn đúng phương pháp phụ thuộc vào bản chất của dữ liệu và mục tiêu cuối cùng. Nếu domain không quá khác biệt so với dữ liệu pre-training ban đầu, fine-tuning là đủ. Nhưng nếu domain có từ vựng và cấu trúc độc đáo, continuous pre-training là cần thiết để mô hình hiểu ngữ cảnh trước khi fine-tuning tác vụ.20

Các Phương pháp Fine-tuning

Instruction Tuning (Tinh chỉnh theo hướng dẫn): Đây là một phương pháp fine-tuning phổ biến, sử dụng các cặp dữ liệu có cấu trúc prompt-response để dạy mô hình cách tuân thủ các hướng dẫn rõ ràng.38 Nó được sử dụng để thích nghi mô hình với một tác vụ mới, thay đổi phong cách phản hồi, hoặc thêm khả năng tuân thủ hướng dẫn.38
Adapting models for specific domains (Thích nghi với miền cụ thể): Mục tiêu của phương pháp này là tùy chỉnh mô hình để nó hoạt động hiệu quả trong một lĩnh vực cụ thể (ví dụ: y tế, pháp lý).39 Quá trình này thường bắt đầu bằng cách đào tạo sơ bộ mô hình trên một lượng lớn dữ liệu chuyên ngành, sau đó fine-tuning trên một tập dữ liệu nhỏ hơn, có gắn nhãn, đặc thù cho tác vụ.40
Transfer Learning (Học chuyển giao): Đây là một phương pháp cốt lõi của fine-tuning. Nó sử dụng các trọng số của một mô hình đã được pre-train làm điểm khởi đầu, cho phép quá trình fine-tuning xây dựng dựa trên nền tảng ngôn ngữ đã có.35 Phương pháp này giúp tăng tốc quá trình đào tạo và cải thiện hiệu suất tổng thể.35

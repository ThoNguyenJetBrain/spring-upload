Bảng so sánh sau đây làm nổi bật sự khác biệt và chức năng của từng dịch vụ AWS AI:
Dịch vụ
Mục đích chính
Đối tượng người dùng
Mức độ kỹ thuật
Lợi ích cốt lõi
Amazon SageMaker JumpStart
Tăng tốc hành trình ML, triển khai mô hình có sẵn
Nhà khoa học dữ liệu, kỹ sư ML
Chuyên nghiệp (Pro-code)
Tùy chỉnh mô hình, an toàn dữ liệu trong VPC
Amazon Bedrock
Xây dựng và mở rộng ứng dụng AI tạo sinh
Nhà phát triển, doanh nghiệp
Thấp đến cao (Low-code/Pro-code)
Lựa chọn mô hình đa dạng, bảo vệ tích hợp
PartyRock
Môi trường học tập và thử nghiệm AI tạo sinh
Mọi người, người mới bắt đầu
Không cần mã (No-code)
Rào cản gia nhập thấp, học prompt engineering
Amazon Q
Trợ lý AI cho doanh nghiệp
Nhân viên doanh nghiệp, nhà phát triển
Không cần mã đến tùy chỉnh (No-code/Custom)
Tăng năng suất, thông tin thống nhất, an toàn


3.2. Lợi Ích của Việc Sử Dụng Các Dịch Vụ AI Tạo Sinh của AWS

Việc xây dựng các ứng dụng AI tạo sinh trên nền tảng AWS mang lại nhiều lợi thế cạnh tranh [1, 4]:
Khả năng tiếp cận và Rào cản gia nhập thấp: Các dịch vụ được quản lý toàn phần như Bedrock và PartyRock giúp người dùng và doanh nghiệp dễ dàng bắt đầu với AI tạo sinh mà không cần đầu tư lớn vào cơ sở hạ tầng hoặc đội ngũ chuyên gia [1, 12].
Hiệu quả và Tốc độ ra thị trường: AWS giúp doanh nghiệp tập trung vào việc phát triển ứng dụng thay vì quản lý cơ sở hạ tầng phức tạp, từ đó tăng tốc độ đưa sản phẩm ra thị trường [34].
Khả năng đáp ứng mục tiêu kinh doanh: Bằng cách cung cấp một loạt các mô hình và công cụ, AWS cho phép doanh nghiệp tùy chỉnh giải pháp để giải quyết các vấn đề kinh doanh cụ thể, từ đó tạo ra giá trị kinh doanh thực sự [36].

3.3. Lợi Ích của Cơ Sở Hạ Tầng AWS cho Ứng Dụng AI Tạo Sinh

Các dịch vụ AI tạo sinh của AWS chỉ có thể hoạt động hiệu quả nhờ vào sức mạnh của cơ sở hạ tầng toàn cầu AWS [37]. Nền tảng hạ tầng này cung cấp:
Bảo mật, Tuân thủ và An toàn: AWS tích hợp bảo mật từ lớp hạ tầng cốt lõi [37]. Mọi lưu lượng dữ liệu trên mạng toàn cầu của AWS đều được mã hóa tự động [37]. AWS cũng cung cấp các công cụ như Bedrock Guardrails [15] và tuân thủ các tiêu chuẩn quốc tế như ISO, SOC, GDPR và HIPAA [15], trực tiếp giải quyết các lo ngại về bảo mật và trách nhiệm đã được đề cập [1, 38].
Hiệu năng và Khả năng mở rộng quy mô (Performance & Scalability): Hạ tầng của AWS được xây dựng để đạt hiệu năng cao, với độ trễ thấp và tỷ lệ mất gói thấp [37]. Khả năng mở rộng linh hoạt của đám mây cho phép doanh nghiệp điều chỉnh tài nguyên theo nhu cầu, tránh lãng phí [37].

3.4. Cân Bằng Chi Phí (Cost Tradeoffs) của Dịch Vụ AWS

Việc lựa chọn mô hình định giá phù hợp là một quyết định chiến lược, đòi hỏi sự cân bằng giữa chi phí, hiệu suất và tính linh hoạt [39]. AWS cung cấp hai mô hình định giá chính cho AI tạo sinh:
Mô hình định giá dựa trên Tokens (On-Demand Pricing):
Khái niệm: Thanh toán theo mức sử dụng thực tế, tính phí trên mỗi token đầu vào và đầu ra [8, 39]. Một token là đơn vị văn bản cơ bản mà mô hình xử lý [39].
Ưu điểm: Linh hoạt, không cần cam kết trả trước [8]. Lý tưởng cho các khối lượng công việc thử nghiệm hoặc không thể đoán trước.
Thực tế: Có một mối liên kết trực tiếp giữa kỹ năng prompt engineering và chi phí. Bằng cách tối ưu hóa câu lệnh để giảm số lượng token, người dùng có thể tiết kiệm đáng kể chi phí [8]. Điều này làm cho prompt engineering không chỉ là một kỹ năng kỹ thuật mà còn là một kỹ năng quản lý chi phí [8].
Mô hình Thông lượng được cung cấp (Provisioned Throughput):
Khái niệm: Mua trước một dung lượng nhất định dưới dạng các đơn vị mô hình [39, 40]. Tính phí theo giờ, với các tùy chọn cam kết 1 hoặc 6 tháng [39, 40].
Ưu điểm: Đảm bảo hiệu suất và độ sẵn sàng cho các khối lượng công việc lớn, ổn định [8].
Nhược điểm: Yêu cầu cam kết, kém linh hoạt hơn so với on-demand [8].
Tiêu chí
Định giá theo Tokens (On-Demand)
Thông lượng được cung cấp (Provisioned Throughput)
Cách tính phí
Trên mỗi token đầu vào và đầu ra
Theo giờ cho mỗi đơn vị mô hình
Tính linh hoạt
Rất cao, không có cam kết
Thấp, yêu cầu cam kết 1 hoặc 6 tháng
Trường hợp sử dụng
Thử nghiệm, khối lượng công việc không ổn định
Khối lượng công việc lớn, ổn định, cần thông lượng đảm bảo


Kết Luận

Qua việc phân tích chuyên sâu các nguyên lý, khả năng, và công nghệ liên quan đến AI tạo sinh, tài liệu này đã cung cấp một lộ trình học tập toàn diện cho Domain 2 của kỳ thi chứng chỉ AWS Certified AI Practitioner. Các khái niệm cốt lõi như tokens, embeddings, và kiến trúc Transformer tạo thành nền tảng kỹ thuật cho toàn bộ lĩnh vực này, trong khi prompt engineering nổi lên như một kỹ năng then chốt để điều khiển các mô hình.
Báo cáo này cũng nhấn mạnh rằng việc triển khai AI tạo sinh không chỉ là một vấn đề kỹ thuật mà còn là một quyết định kinh doanh chiến lược. Các doanh nghiệp cần hiểu rõ cả ưu điểm (tính thích ứng, năng suất) và nhược điểm (ảo giác, thiếu diễn giải) để đưa ra các quyết định sáng suốt. Giá trị của AI tạo sinh cần được đo lường bằng các chỉ số kinh doanh cụ thể, không chỉ là hiệu suất mô hình.
Cuối cùng, AWS đã định vị mình là một đối tác đáng tin cậy bằng cách cung cấp một hệ sinh thái dịch vụ hoàn chỉnh (Bedrock, SageMaker JumpStart, PartyRock, Amazon Q) được xây dựng trên một cơ sở hạ tầng toàn cầu mạnh mẽ, đảm bảo bảo mật, tuân thủ và khả năng mở rộng quy mô. Sự hiểu biết về các mô hình định giá khác nhau của AWS (dựa trên token và thông lượng được cung cấp) là yếu tố then chốt để quản lý chi phí hiệu quả và đưa các ứng dụng AI tạo sinh vào sản xuất một cách bền vững.

Tài liệu học tập chuyên sâu cho AWS Certified AI Practitioner: Domain 3 - Ứng dụng các Mô hình Nền tảng

AWS Certified AI Practitioner là một chứng chỉ được thiết kế để xác nhận kiến thức của các cá nhân về AI, Machine Learning (ML), và AI tạo sinh, cùng với các dịch vụ AWS liên quan.1 Bài thi chứng chỉ này không yêu cầu một vai trò công việc cụ thể, nhưng nhắm đến những người có kinh nghiệm khoảng 6 tháng làm quen với các công nghệ AI/ML trên AWS.1 Các ứng viên thường là chuyên gia phân tích kinh doanh, quản lý dự án, hoặc các chuyên gia marketing, những người sử dụng các giải pháp này mà không nhất thiết phải tự xây dựng chúng.2
Domain 3, “Applications of Foundation Models,” chiếm trọng số 28% tổng số điểm của bài thi, là phần có tỷ lệ cao nhất.1 Điều này nhấn mạnh tầm quan trọng của việc hiểu cách thiết kế, xây dựng, và đánh giá các ứng dụng AI tạo sinh dựa trên mô hình nền tảng (Foundation Models - FMs) trong môi trường thực tế. Tài liệu này cung cấp một cái nhìn toàn diện và chuyên sâu về các chủ đề của Domain 3, tập trung vào Amazon Bedrock, một dịch vụ trung tâm giúp đơn giản hóa việc phát triển các ứng dụng AI tạo sinh.3 Amazon Bedrock cung cấp quyền truy cập vào một loạt các mô hình hàng đầu từ các nhà cung cấp như AI21 Labs, Anthropic, Cohere, Meta, và Stability AI, cũng như các mô hình Amazon Titan do AWS phát triển.4 Việc sử dụng một API duy nhất của Bedrock giúp giảm bớt đáng kể gánh nặng quản lý cơ sở hạ tầng, cho phép các nhà phát triển tập trung vào việc xây dựng các ứng dụng có giá trị kinh doanh cao.3

Phần 1: Các Cân Nhắc Thiết Kế Ứng Dụng Mô Hình Nền tảng (Domain 3.1)


Tiêu chí lựa chọn Mô hình Nền tảng

Việc lựa chọn một mô hình nền tảng phù hợp là một quyết định chiến lược, không chỉ đơn thuần là kỹ thuật. Mặc dù các tổ chức thường bắt đầu với ba tiêu chí chính là độ chính xác (accuracy), độ trễ (latency), và chi phí (cost), một khung đánh giá toàn diện hơn là cần thiết để xác định hiệu suất thực tế của mô hình trong thế giới thực.6 Quá trình này cần xem xét bốn khía cạnh cốt lõi.
Hiệu suất Tác vụ (Task Performance): Đánh giá khả năng của mô hình trong việc thực hiện các nhiệm vụ cụ thể. Điều này bao gồm khả năng tuân thủ hướng dẫn một cách chính xác (instruction following fidelity), tính nhất quán của đầu ra qua nhiều lần chạy với cùng một prompt (output consistency), và sự hiểu biết chuyên sâu về một miền cụ thể (domain-specific knowledge) dựa trên dữ liệu đào tạo.6 Khả năng suy luận (reasoning capabilities) cũng rất quan trọng, đặc biệt là khả năng thực hiện suy luận logic, suy luận nhân quả, và giải quyết vấn đề nhiều bước.6
Đặc điểm Kiến trúc (Architectural Characteristics): Các yếu tố kỹ thuật như số lượng tham số (parameter count hoặc model size) ảnh hưởng trực tiếp đến hiệu suất và hiệu quả.6 Các mô hình lớn hơn thường có nhiều khả năng hơn nhưng cũng đòi hỏi tài nguyên tính toán lớn hơn, dẫn đến chi phí và độ trễ suy luận cao hơn.6 Cần xem xét thành phần dữ liệu đào tạo (training data composition), loại hình (modality) mà mô hình có thể xử lý (ví dụ: văn bản, hình ảnh, âm thanh), và kích thước cửa sổ ngữ cảnh (context window size).6 Các kiến trúc khác nhau như
decoder-only (tối ưu cho tạo văn bản) và encoder-decoder (phù hợp cho dịch thuật và tóm tắt) cũng cần được cân nhắc.6
Cân nhắc Vận hành (Operational Considerations): Đây là các yếu tố quyết định tính khả thi và hiệu quả chi phí của một triển khai AI trong thực tế. Chúng bao gồm thông lượng (throughput) và độ trễ (latency), cấu trúc chi phí (cost structure) theo token hoặc thông lượng dự phòng (provisioned throughput), các tùy chọn tùy chỉnh (customization options), và khả năng tích hợp dễ dàng vào hệ thống hiện có (ease of integration).6 Vấn đề bảo mật, bao gồm mã hóa dữ liệu, kiểm soát truy cập và quản lý lỗ hổng, là một cân nhắc quan trọng khi xử lý dữ liệu nhạy cảm.6
Thuộc tính AI có Trách nhiệm (Responsible AI Attributes): Một yếu tố không thể thiếu trong kinh doanh hiện đại là trách nhiệm. Điều này đòi hỏi phải đánh giá khuynh hướng tạo ra thông tin sai lệch (hallucination propensity), đo lường độ thiên lệch (bias measurements), hiệu quả của các rào chắn an toàn (safety guardrails) trong việc ngăn chặn nội dung độc hại, và các khía cạnh về quyền riêng tư và pháp lý.6
Lựa chọn mô hình không chỉ là một bài toán kỹ thuật mà còn là một quyết định chiến lược. Một ứng dụng yêu cầu độ chính xác tuyệt đối trong lĩnh vực pháp lý sẽ ưu tiên các mô hình có kiến thức chuyên ngành sâu, trong khi một chatbot hỗ trợ khách hàng sẽ cần các mô hình có khả năng tuân thủ hướng dẫn và độ trễ thấp.6 Quyết định này sẽ định hình toàn bộ kiến trúc và chi phí của giải pháp. Do đó, việc xác định chính xác các yêu cầu chức năng và phi chức năng của ứng dụng ở giai đoạn đầu là rất quan trọng để đưa ra quyết định đúng đắn.6

Tác động của các tham số suy luận (Inference Parameters)

Các tham số suy luận cho phép điều chỉnh hành vi của mô hình mà không cần phải đào tạo lại. Hai trong số các tham số quan trọng nhất là temperature và input/output length.
Temperature: Tham số này kiểm soát tính ngẫu nhiên và sáng tạo của đầu ra bằng cách điều chỉnh phân phối xác suất của các token.7 Một giá trị
temperature thấp (ví dụ: nhỏ hơn 1) làm cho mô hình trở nên "bảo thủ" hơn, ưu tiên các token có xác suất cao nhất. Điều này tạo ra các câu trả lời mang tính dự đoán, chính xác và nhất quán, lý tưởng cho các tác vụ yêu cầu tính xác thực cao như tóm tắt văn bản hoặc trả lời câu hỏi dựa trên tài liệu.7 Ngược lại, một giá trị
temperature cao (ví dụ: lớn hơn 1) làm phẳng phân phối xác suất, cho phép mô hình có nhiều cơ hội chọn các token ít phổ biến hơn. Điều này khuyến khích sự sáng tạo và đa dạng, phù hợp cho các tác vụ như viết kịch bản, làm thơ, hoặc tạo nội dung quảng cáo.7
Chiều dài Input/Output: Chiều dài đầu vào (prompt length), được đo bằng token, là một yếu tố ảnh hưởng trực tiếp đến đầu ra của mô hình.9 Các prompt ngắn thường tạo ra phản hồi súc tích và tập trung, trong khi các prompt dài hơn có thể tạo ra các phản hồi toàn diện và chi tiết hơn.9 Tuy nhiên, một prompt quá dài có thể làm mô hình trở nên quá tải hoặc tạo ra thông tin không liên quan.9 Chiều dài đầu ra cũng là một tham số quan trọng, giới hạn số lượng token mà mô hình có thể tạo ra. Một quan sát đáng chú ý là khi độ dài đầu vào tăng lên, khả năng suy luận của nhiều mô hình có thể bị suy giảm.10 Thay vì suy luận từng bước một cách logic, mô hình có thể có xu hướng đưa ra câu trả lời trước rồi mới suy luận sau. Điều này làm giảm khả năng trích xuất các sự kiện chính và xây dựng một chuỗi suy luận chặt chẽ của mô hình.10 Điều này cho thấy việc cung cấp quá nhiều thông tin không phải lúc nào cũng tốt. Do đó, các kỹ thuật như RAG và Chain-of-Thought (CoT) trở nên cần thiết, vì chúng giúp mô hình tập trung vào thông tin quan trọng nhất, thay vì cố gắng xử lý toàn bộ tài liệu cùng một lúc.11

Retrieval Augmented Generation (RAG)

Retrieval Augmented Generation (RAG) là một kỹ thuật mạnh mẽ giải quyết một trong những hạn chế lớn nhất của các mô hình nền tảng: kiến thức tĩnh.13 RAG hoạt động bằng cách truy xuất dữ liệu từ các nguồn bên ngoài, chẳng hạn như cơ sở dữ liệu nội bộ của công ty, và sử dụng dữ liệu đó để bổ sung (augment) cho prompt ban đầu.13 Quá trình này giúp mô hình truy cập được thông tin cập nhật và độc quyền, từ đó tạo ra các phản hồi chính xác và phù hợp hơn, đồng thời giảm đáng kể hiện tượng "hallucinations" (mô hình tạo ra thông tin sai lệch hoặc không có thật).5
Amazon Bedrock Knowledge Bases là một dịch vụ quản lý hoàn toàn (fully managed) giúp triển khai quy trình RAG một cách liền mạch.13 Dịch vụ này tự động hóa toàn bộ quy trình từ việc thu nạp dữ liệu từ các nguồn như Amazon S3, Salesforce, và Confluence, đến việc chuyển đổi nội dung thành các khối văn bản (text chunks) và sau đó thành các vector embeddings.13 Các vector embeddings này sau đó được lưu trữ trong một cơ sở dữ liệu vector để tìm kiếm nhanh chóng.13 Amazon Bedrock Knowledge Bases cũng tích hợp sẵn tính năng quản lý ngữ cảnh phiên làm việc và ghi nhận nguồn thông tin, giúp các ứng dụng trở nên đáng tin cậy hơn.13

Lưu trữ Embeddings với Vector Databases trên AWS

Vector databases là các hệ thống lưu trữ được thiết kế chuyên biệt để quản lý và truy xuất các vector embeddings, cho phép thực hiện tìm kiếm ngữ nghĩa (semantic search) và tìm kiếm lân cận (k-nearest neighbor - k-NN) một cách hiệu quả.14 Các dịch vụ AWS đang tích hợp khả năng này vào các sản phẩm cơ sở dữ liệu hiện có để đơn giản hóa kiến trúc ứng dụng.
Amazon OpenSearch Service: Đây là cơ sở dữ liệu vector được AWS khuyến nghị cho Amazon Bedrock.14 Nó cung cấp một giải pháp có khả năng mở rộng cao và hiệu suất mạnh mẽ, hỗ trợ nhiều loại hình tìm kiếm như
hybrid search (kết hợp tìm kiếm từ khóa và vector), multi-modal search, và conversational search.14
Amazon Aurora và Amazon RDS for PostgreSQL: Các dịch vụ cơ sở dữ liệu quan hệ này hỗ trợ vector search thông qua extension mã nguồn mở pgvector.15 Điều này cho phép các nhà phát triển lưu trữ dữ liệu quan hệ truyền thống và vector embeddings trong cùng một cơ sở dữ liệu, tận dụng các công cụ và quy trình làm việc quen thuộc mà không cần quản lý một hệ thống riêng biệt.16
Amazon DocumentDB (với tính tương thích MongoDB): Dịch vụ cơ sở dữ liệu tài liệu này kết hợp sự linh hoạt của dữ liệu JSON với sức mạnh của vector search.18
Amazon Neptune: Là một dịch vụ cơ sở dữ liệu đồ thị, Amazon Neptune hỗ trợ Neptune Analytics để lưu trữ embeddings và thực hiện tìm kiếm hiệu quả trên dữ liệu đồ thị.13